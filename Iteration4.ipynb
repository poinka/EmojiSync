{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4th Iteration\n",
        "In this iteration, we intentionally do not separate text and emojis. The goal is to evaluate how well a general-purpose roberta-base-go_emotions model can handle mixed inputs — containing both natural language and emojis — without any preprocessing or transformation.\n",
        "\n",
        "Main steps:\n",
        "\n",
        "1. Keep emojis in place and combine them with raw text\n",
        "\n",
        "2. Tokenize this mixed input directly\n",
        "\n",
        "3. Generate embeddings using a text classification model\n",
        "\n",
        "4. Measure how well the model captures the emotional meaning"
      ],
      "metadata": {
        "id": "H78vChJMN-NK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRXudhw3IbTY"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import faiss\n",
        "from tabulate import tabulate\n",
        "import torch"
      ],
      "metadata": {
        "id": "X4dkWhB6Ilt9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare csv files"
      ],
      "metadata": {
        "id": "rGx8dqcWLrbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dfs = []\n",
        "for file_name in os.listdir(\"archive\"):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(\"archive\", file_name)\n",
        "        try:\n",
        "            # Read CSV with appropriate encoding\n",
        "            df = pd.read_csv(file_path)\n",
        "            all_dfs.append(df)\n",
        "            print(f\"Loaded {file_name} with {len(df)} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "print(f\"Combined dataset size: {len(combined_df)} rows\")\n",
        "print()\n",
        "combined_df.head()\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15iwgckYImT2",
        "outputId": "5b83011e-507f-4242-bd7a-d48a44bc7d5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rabbit.csv with 20000 rows\n",
            "Loaded thinking_face.csv with 20000 rows\n",
            "Loaded hot_face.csv with 20000 rows\n",
            "Loaded smiling_face_with_heart-eyes.csv with 20000 rows\n",
            "Loaded fire.csv with 20000 rows\n",
            "Loaded folded_hands.csv with 20000 rows\n",
            "Loaded fearful_face.csv with 20000 rows\n",
            "Loaded rolling_on_the_floor_laughing.csv with 20000 rows\n",
            "Loaded saluting_face.csv with 20000 rows\n",
            "Loaded white_heart.csv with 20000 rows\n",
            "Loaded grinning_face_with_sweat.csv with 20000 rows\n",
            "Loaded thumbs_up.csv with 20000 rows\n",
            "Loaded cooking.csv with 20000 rows\n",
            "Loaded face_with_steam_from_nose.csv with 20000 rows\n",
            "Loaded rabbit_face.csv with 20000 rows\n",
            "Loaded sparkles.csv with 20000 rows\n",
            "Loaded smiling_face_with_halo.csv with 20000 rows\n",
            "Loaded ghost.csv with 20000 rows\n",
            "Loaded hatching_chick.csv with 20000 rows\n",
            "Error loading backhand_index_pointing_right.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Error loading red_heart.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Loaded partying_face.csv with 20001 rows\n",
            "Loaded smiling_face_with_sunglasses.csv with 20000 rows\n",
            "Loaded egg.csv with 20001 rows\n",
            "Loaded middle_finger.csv with 20000 rows\n",
            "Loaded clown_face.csv with 20000 rows\n",
            "Loaded melting_face.csv with 20000 rows\n",
            "Loaded smiling_face.csv with 20000 rows\n",
            "Loaded pile_of_poo.csv with 20000 rows\n",
            "Loaded sun.csv with 20000 rows\n",
            "Loaded check_mark_button.csv with 20000 rows\n",
            "Loaded smiling_face_with_hearts.csv with 20000 rows\n",
            "Loaded eyes.csv with 20000 rows\n",
            "Loaded face_with_tears_of_joy.csv with 20000 rows\n",
            "Loaded enraged_face.csv with 20000 rows\n",
            "Loaded loudly_crying_face.csv with 20000 rows\n",
            "Loaded face_holding_back_tears.csv with 20000 rows\n",
            "Loaded party_popper.csv with 20000 rows\n",
            "Loaded face_savoring_food.csv with 20000 rows\n",
            "Loaded skull.csv with 20000 rows\n",
            "Loaded smiling_face_with_tear.csv with 20000 rows\n",
            "Loaded winking_face.csv with 20000 rows\n",
            "Loaded check_mark.csv with 20000 rows\n",
            "Combined dataset size: 820002 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load dataset and filter rows with emojis"
      ],
      "metadata": {
        "id": "aOeEqp0NLyet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_dataset.csv')\n",
        "df['has_emoji'] = df['Text'].apply(lambda x: bool(emoji.emoji_count(str(x))))\n",
        "emoji_rich_df = df[df['has_emoji']].copy()\n",
        "\n",
        "# Select 100 random records\n",
        "df_subset = emoji_rich_df.sample(n=100, random_state=100).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ovL5K1iKItpe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create embeddings of the text together with emoji"
      ],
      "metadata": {
        "id": "4I2a6lk9LznM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "def get_embedding(text, tokenizer, model, device='cpu'):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states[-1]\n",
        "    cls_embedding = hidden_states[:, 0, :].squeeze().cpu().numpy()\n",
        "    return cls_embedding.astype(np.float32)"
      ],
      "metadata": {
        "id": "FYHy0Qe8I4ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Faiss index"
      ],
      "metadata": {
        "id": "XN-NsqkHMOin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_df = pd.read_csv('selected_quotes_embeddings.csv')\n",
        "\n",
        "def parse_embedding(emb):\n",
        "    try:\n",
        "        if isinstance(emb, str):\n",
        "            return np.array(eval(emb), dtype=np.float32)\n",
        "        return np.array(emb, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "index = faiss.IndexFlatIP(768)\n",
        "\n",
        "for i in range(0, len(quotes_df), 10000):\n",
        "    chunk = quotes_df.iloc[i:i + 10000]\n",
        "    chunk_embeddings = [parse_embedding(emb) for emb in chunk['embeddings']]\n",
        "    chunk_embeddings = [emb for emb in chunk_embeddings if emb is not None and emb.shape == (768,)]\n",
        "    if chunk_embeddings:\n",
        "        chunk_array = np.vstack(chunk_embeddings)\n",
        "        faiss.normalize_L2(chunk_array)\n",
        "        index.add(chunk_array)\n",
        "    print(f\"Processed chunk {i // 10000 + 1}/{len(quotes_df) // 10000 + 1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oohsSKOjJB7a",
        "outputId": "d49c7792-4c41-4784-a333-eb60c7a8c8fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1/11\n",
            "Processed chunk 2/11\n",
            "Processed chunk 3/11\n",
            "Processed chunk 4/11\n",
            "Processed chunk 5/11\n",
            "Processed chunk 6/11\n",
            "Processed chunk 7/11\n",
            "Processed chunk 8/11\n",
            "Processed chunk 9/11\n",
            "Processed chunk 10/11\n",
            "Processed chunk 11/11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_quotes(query_embedding, k=5):\n",
        "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
        "    distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "    return distances[0], indices[0]"
      ],
      "metadata": {
        "id": "_KKerQ5LJF-9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation"
      ],
      "metadata": {
        "id": "_swbFvzTMSad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for idx, row in df_subset.iterrows():\n",
        "    input_text = row['Text']\n",
        "    embedding = get_embedding(input_text, tokenizer, model)\n",
        "    distances, indices = search_similar_quotes(embedding)\n",
        "    avg_similarity = np.mean(distances)\n",
        "    quotes = [quotes_df.iloc[i]['quote'] for i in indices]\n",
        "\n",
        "    results.append({\n",
        "        'text': input_text,\n",
        "        'avg_similarity': avg_similarity,\n",
        "        'quotes': quotes\n",
        "    })"
      ],
      "metadata": {
        "id": "xlt9WzeZJ4da"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Detailed Results ===\")\n",
        "for idx, res in enumerate(results[:10], 1):\n",
        "    print(f\"\\nQuery {idx}: {res['text']}\")\n",
        "    print(f\"Average Cosine Similarity: {res['avg_similarity']:.4f}\")\n",
        "    print(\"Top-K Quotes:\")\n",
        "    for i, quote in enumerate(res['quotes']):\n",
        "        print(f\"{i+1}. {quote}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgasfm4WKty5",
        "outputId": "d714e872-a932-40e3-885e-d405168dc245"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Detailed Results ===\n",
            "\n",
            "Query 1: @z388z @IFLTV Yh I had to search my house and take a picture of it 🤡\n",
            "Average Cosine Similarity: 0.7074\n",
            "Top-K Quotes:\n",
            "1. Yes, I kidnapped that Lindberg baby.\n",
            "2. Yeah and purple monkeys fly from my ass at dawn.\n",
            "3. I make movies for teenage boys. Oh dear, what a crime.\n",
            "4. I ransack public libraries, and find them full of sunk treasure.\n",
            "5. I smell blood and an era of prominent madmen.\n",
            "\n",
            "Query 2: just booked hotel for 5sos in prague even when I'm not sure if i get my ticket \n",
            "SO hoping for good luck while ticketing 🤞🤞🤞\n",
            "Average Cosine Similarity: 0.8628\n",
            "Top-K Quotes:\n",
            "1. I'm optimistic about the possibility of having a positive attitude\n",
            "2. Whatever my path, I have faith I will end up where I need to be.\n",
            "3. With high hope and optimism, start swimming with time.\n",
            "4. To travel hopefully is better than to have arrived.\n",
            "5. I am prepared for the worst, but hope for the best.\n",
            "\n",
            "Query 3: Stateside tips are live 🇺🇸\n",
            "\n",
            "✔️ Tampa Bay Downs\n",
            "✔️ Mahoning Valley\n",
            "✔️ Philadelphia\n",
            "✔️ Will Rogers Downs\n",
            "🏝️ Turf Paradise\n",
            "\n",
            "💰 US Double pays 12/1!\n",
            "\n",
            "📝 PLUS 35/1 Notebook Double\n",
            "\n",
            "Get all my tips in America 🌎\n",
            "https://t.co/tleEocHuYc\n",
            "Average Cosine Similarity: 0.8096\n",
            "Top-K Quotes:\n",
            "1. Unconditional Love has conditions.\n",
            "2. Words are sigils that can hide the coded language of your Soul. ☥\n",
            "3. Love is immune to whatever circumstance.\n",
            "4. Love is the answer.\n",
            "5. Love is keeping the promise anyway.\n",
            "\n",
            "Query 4: @Tofe906 20 😨\n",
            "Average Cosine Similarity: 0.7373\n",
            "Top-K Quotes:\n",
            "1. Caricature: putting the face of a joke upon the body of a truth.\n",
            "2. One man's theology is another man's belly laugh.\n",
            "3. Faith slips - and laughs, and rallies\n",
            "4. He that jokes confesses.\n",
            "5. Like trying to keep a fatman out of the refrigerator. 'Lila\n",
            "\n",
            "Query 5: Order bento cake for ur twins yummy 😋 cakes https://t.co/HaqQgDO35Y\n",
            "Average Cosine Similarity: 0.7413\n",
            "Top-K Quotes:\n",
            "1. Like Desserts, books come in all kinds of tasty treats!\n",
            "2. I like to walk in the woods and see what Mother Nature is wearing.\n",
            "3. Today is the day you've been waiting for.\n",
            "4. Christmas is a stocking stuffed with sugary goodness.\n",
            "5. When you first learn to love hell  you will be in heaven.\n",
            "\n",
            "Query 6: 😋 We're thrilled to report that our keto chocolate milk tea boba was a huge hit at the expo! \n",
            "\n",
            "🍫 Customers who sampled it raved about the delectable taste, with many likening it to the indulgent flavors of a fudgesicle. \n",
            "\n",
            "#boba #keto #ketoboba #bobatea #bobalife #bobamilktea https://t.co/H1Yq8q4HUF\n",
            "Average Cosine Similarity: 0.8331\n",
            "Top-K Quotes:\n",
            "1. I got this delicious bottle of perfume called Fabreze\n",
            "2. Gratitude goes beyond a delight for a gift\n",
            "3. Truly, there is magic in fairy\n",
            "4. Ah, music, a magic beyond all we do here.\n",
            "5. Morning is the best time to see movies.\n",
            "\n",
            "Query 7: why leaving comments on my post with hashtags. y'all look like bots with copy paste.😂\n",
            "Average Cosine Similarity: 0.7120\n",
            "Top-K Quotes:\n",
            "1. Isn't it funny how ignorance is the source of strength of so many?\n",
            "2. Madness has no sense of humour\n",
            "3. Isn't it funny how we live inside the lies we believe?\n",
            "4. If you think training is expensive, try ignorance.\n",
            "5. Human: That's stupid. Isn't there grass on both sides?\n",
            "\n",
            "Query 8: @C0nstant_g God has his favorites👍... Xd\n",
            "Average Cosine Similarity: 0.8115\n",
            "Top-K Quotes:\n",
            "1. If you like tiger cubs,you got to get into the den!\n",
            "2. She liked the words\n",
            "3. Novels and gardens,\" she says. \"I like to move from plot to plot.\n",
            "4. .burn, everybody likes to watch the fire..\n",
            "5. Disco is music for dancing, and people will always want to dance.\n",
            "\n",
            "Query 9: @backport893 Thank's 👍❤️\n",
            "Average Cosine Similarity: 0.8814\n",
            "Top-K Quotes:\n",
            "1. I thank God for supply me with my deepest needs\n",
            "2. Lord I thank you for the timely provisions\n",
            "3. I am grateful to all my readers\n",
            "4. My greatest gratitude\n",
            "5. I am thankful for all circumstances.\n",
            "\n",
            "Query 10: #TrustArmy Upd: the validation of your report is in full swing 🦾\n",
            "\n",
            "Here's what we've done to speed up the process:\n",
            "✔️ Onboarded the first validators \n",
            "\n",
            "✔️ Developed a new approach to the validation process to eliminate the bottleneck discovered in Beta\n",
            "Average Cosine Similarity: 0.8613\n",
            "Top-K Quotes:\n",
            "1. People buy into the leader before they buy into the vision.\n",
            "2. Push-ups, sit-ups, and a strict diet of raisins. That's my plan.\n",
            "3. Writing comes more easily if you have something to say.\n",
            "4. The little things are important, Mr. Wind-Up Bird,\n",
            "5. Every negative effect needs a positive cure (solution)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis\n",
        "Without removing or transforming emojis, the model showed a high correspondence of emotions between queries and quotes. For most examples, the average cosine similarity was above 0.8, indicating successful perception of mixed texts.\n",
        "\n",
        "Emojis expressing emotions (🤞, 😋, ❤️) clearly strengthened the emotional signal: the model selected quotes with a similar mood. Even in ironic or absurd examples (🤡), the results retained thematic relevance. In more neutral cases (e.g., with factual content), the correspondence of emotions was less clear, but still acceptable.\n",
        "\n",
        "The model successfully copes with emojis in the text, even without special processing."
      ],
      "metadata": {
        "id": "Pd_aRIcIcSFi"
      }
    }
  ]
}