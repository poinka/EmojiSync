{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Iteration\n",
        "In this iteration we embed text and emoji separately to preserve their unique signals, then combine them using a linear layer to match dimensionality.\n",
        "The main purpose is to check if separate processing improves emotional understanding compared to demojizing.\n",
        "\n",
        "Main steps:\n",
        "\n",
        "1. Choose emoji-rich dataset\n",
        "\n",
        "2. Create text and emoji embeddings independently\n",
        "\n",
        "3. Project emoji embeddings to same dimension as text embeddings\n",
        "\n",
        "4. Combine embeddings via a linear layer\n",
        "\n",
        "5. Search most relevant quotes using FAISS index\n",
        "\n",
        "6. Compare results from Emoji2Vec and Emojinal, and evaluate quality vs. previous iterations"
      ],
      "metadata": {
        "id": "T8bOqdeEH7qC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz6urh3o6G8Q",
        "outputId": "807e8097-e886-4fb6-d1a5-871e322b5f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install faiss-cpu\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import faiss\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import ast\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "jP6dl3OuH_q9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare csv files"
      ],
      "metadata": {
        "id": "yB-BhoFS4GhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dfs = []\n",
        "for file_name in os.listdir(\"archive\"):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(\"archive\", file_name)\n",
        "        try:\n",
        "            # Read CSV with appropriate encoding\n",
        "            df = pd.read_csv(file_path)\n",
        "            all_dfs.append(df)\n",
        "            print(f\"Loaded {file_name} with {len(df)} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "print(f\"Combined dataset size: {len(combined_df)} rows\")\n",
        "print()\n",
        "combined_df.head()\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QQJNBuTIJRu",
        "outputId": "1c789344-7a62-4b01-cbc2-d29e9eade702"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rabbit.csv with 20000 rows\n",
            "Loaded thinking_face.csv with 20000 rows\n",
            "Loaded hot_face.csv with 20000 rows\n",
            "Loaded smiling_face_with_heart-eyes.csv with 20000 rows\n",
            "Loaded fire.csv with 20000 rows\n",
            "Loaded folded_hands.csv with 20000 rows\n",
            "Loaded fearful_face.csv with 20000 rows\n",
            "Loaded rolling_on_the_floor_laughing.csv with 20000 rows\n",
            "Loaded saluting_face.csv with 20000 rows\n",
            "Loaded white_heart.csv with 20000 rows\n",
            "Loaded grinning_face_with_sweat.csv with 20000 rows\n",
            "Loaded thumbs_up.csv with 20000 rows\n",
            "Loaded cooking.csv with 20000 rows\n",
            "Loaded face_with_steam_from_nose.csv with 20000 rows\n",
            "Loaded rabbit_face.csv with 20000 rows\n",
            "Loaded sparkles.csv with 20000 rows\n",
            "Loaded smiling_face_with_halo.csv with 20000 rows\n",
            "Loaded ghost.csv with 20000 rows\n",
            "Loaded hatching_chick.csv with 20000 rows\n",
            "Error loading backhand_index_pointing_right.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Error loading red_heart.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Loaded partying_face.csv with 20001 rows\n",
            "Loaded smiling_face_with_sunglasses.csv with 20000 rows\n",
            "Loaded egg.csv with 20001 rows\n",
            "Loaded middle_finger.csv with 20000 rows\n",
            "Loaded clown_face.csv with 20000 rows\n",
            "Loaded melting_face.csv with 20000 rows\n",
            "Loaded smiling_face.csv with 20000 rows\n",
            "Loaded pile_of_poo.csv with 20000 rows\n",
            "Loaded sun.csv with 20000 rows\n",
            "Loaded check_mark_button.csv with 20000 rows\n",
            "Loaded smiling_face_with_hearts.csv with 20000 rows\n",
            "Loaded eyes.csv with 20000 rows\n",
            "Loaded face_with_tears_of_joy.csv with 20000 rows\n",
            "Loaded enraged_face.csv with 20000 rows\n",
            "Loaded loudly_crying_face.csv with 20000 rows\n",
            "Loaded face_holding_back_tears.csv with 20000 rows\n",
            "Loaded party_popper.csv with 20000 rows\n",
            "Loaded face_savoring_food.csv with 20000 rows\n",
            "Loaded skull.csv with 20000 rows\n",
            "Loaded smiling_face_with_tear.csv with 20000 rows\n",
            "Loaded winking_face.csv with 20000 rows\n",
            "Loaded check_mark.csv with 20000 rows\n",
            "Combined dataset size: 820002 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load dataset and filter rows with emojis"
      ],
      "metadata": {
        "id": "kv1iASAw4AMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_dataset.csv')\n",
        "df['has_emoji'] = df['Text'].apply(lambda x: bool(emoji.emoji_count(str(x))))\n",
        "emoji_rich_df = df[df['has_emoji']].copy()\n",
        "\n",
        "# Select 100 random records\n",
        "df_subset = emoji_rich_df.sample(n=100, random_state=100).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VyA0oFXNIQlz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "text_model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "def get_text_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs, output_hidden_states=True)\n",
        "    return outputs.hidden_states[-1][0, 0, :].detach().numpy().astype(np.float32)\n"
      ],
      "metadata": {
        "id": "sAEm6KN-VFyF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create embeddings of the emojis and linearize it"
      ],
      "metadata": {
        "id": "Y-66HwT8-FTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Emoji2Vec and Emojinal\n",
        "emoji2vec = KeyedVectors.load_word2vec_format(\"emoji2vec.bin\", binary=True)\n",
        "emojinal = KeyedVectors.load_word2vec_format(\"emojional.bin\", binary=True)\n",
        "\n",
        "\n",
        "# Projection layers\n",
        "emoji2vec_proj = nn.Linear(300, 768)\n",
        "emojinal_proj = nn.Linear(300, 768)\n",
        "combine_proj = nn.Linear(768 + 768, 768)\n",
        "\n",
        "def extract_emojis(text):\n",
        "    return [c for c in text if c in emoji.EMOJI_DATA]\n",
        "\n",
        "def get_emoji_embedding(text, embedding_dict, projection_layer, input_dim):\n",
        "    emojis = [ch for ch in text if ch in embedding_dict]\n",
        "    # If there is no such emoji, we return the zero vector\n",
        "    if not emojis:\n",
        "        return np.zeros((768,), dtype=np.float32)\n",
        "    # Averaging embeddings\n",
        "    vectors = [embedding_dict[e] for e in emojis if e in embedding_dict]\n",
        "    avg_emb = np.mean(vectors, axis=0)\n",
        "    # Convert to torch and run through the dimension reduction layer\n",
        "    avg_emb_tensor = torch.tensor(avg_emb, dtype=torch.float32).unsqueeze(0)  # (1, input_dim)\n",
        "    projected = projection_layer(avg_emb_tensor).squeeze(0).detach().numpy()  # (768,)\n",
        "    return projected\n",
        "\n",
        "\n",
        "def combine_embeddings(text_emb, emoji_emb):\n",
        "    text_tensor = torch.tensor(text_emb, dtype=torch.float32)\n",
        "    emoji_tensor = torch.tensor(emoji_emb, dtype=torch.float32)\n",
        "    combined = torch.cat([text_tensor, emoji_tensor], dim=0)\n",
        "    return combine_proj(combined).detach().numpy()\n"
      ],
      "metadata": {
        "id": "JcLVcXy2Wxg5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji2vec[\"ðŸ˜¢\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHjncrVZAfC",
        "outputId": "9e50edb7-2db7-4982-edcd-210215256f0c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Loading citations and Faiss index"
      ],
      "metadata": {
        "id": "7igibpEj94-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_df = pd.read_csv('selected_quotes_embeddings.csv')\n",
        "\n",
        "def parse_embedding(emb):\n",
        "    try:\n",
        "        if isinstance(emb, str):\n",
        "            return np.array(ast.literal_eval(emb), dtype=np.float32)\n",
        "        return np.array(emb, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "index = faiss.IndexFlatIP(768)\n",
        "for i in range(0, len(quotes_df), 10000):\n",
        "    chunk = quotes_df.iloc[i:i + 10000]\n",
        "    chunk_embeddings = [parse_embedding(emb) for emb in chunk['embeddings']]\n",
        "    chunk_embeddings = [emb for emb in chunk_embeddings if emb is not None and emb.shape == (768,)]\n",
        "    if chunk_embeddings:\n",
        "        chunk_array = np.vstack(chunk_embeddings)\n",
        "        faiss.normalize_L2(chunk_array)\n",
        "        index.add(chunk_array)\n",
        "    print(f\"Processed chunk {i // 10000 + 1}/{len(quotes_df) // 10000 + 1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqR4DUG9XCgz",
        "outputId": "4386f6cd-c33d-486f-b6ce-d06978b6c620"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1/11\n",
            "Processed chunk 2/11\n",
            "Processed chunk 3/11\n",
            "Processed chunk 4/11\n",
            "Processed chunk 5/11\n",
            "Processed chunk 6/11\n",
            "Processed chunk 7/11\n",
            "Processed chunk 8/11\n",
            "Processed chunk 9/11\n",
            "Processed chunk 10/11\n",
            "Processed chunk 11/11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_quotes(query_embedding, k=5):\n",
        "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
        "    distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "    return distances[0], indices[0]"
      ],
      "metadata": {
        "id": "j0W7C_oxXBxs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation"
      ],
      "metadata": {
        "id": "BYAj6KEl-nGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for idx, row in tqdm(df_subset.iterrows(), total=len(df_subset)):\n",
        "    input_text = row['Text']\n",
        "\n",
        "    text_emb = get_text_embedding(input_text)\n",
        "\n",
        "    emoji_emb_2v = get_emoji_embedding(input_text, emoji2vec, emoji2vec_proj, 300)\n",
        "    final_emb_2v = combine_embeddings(text_emb, emoji_emb_2v)\n",
        "    dists_2v, idxs_2v = search_similar_quotes(final_emb_2v)\n",
        "    quotes_2v = [quotes_df.iloc[i]['quote'] for i in idxs_2v]\n",
        "\n",
        "    emoji_emb_ej = get_emoji_embedding(input_text, emojinal, emojinal_proj, 300)\n",
        "    final_emb_ej = combine_embeddings(text_emb, emoji_emb_ej)\n",
        "    dists_ej, idxs_ej = search_similar_quotes(final_emb_ej)\n",
        "    quotes_ej = [quotes_df.iloc[i]['quote'] for i in idxs_ej]\n",
        "\n",
        "    results.append({\n",
        "        'text': input_text,\n",
        "        'avg_similarity_emoji2vec': np.mean(dists_2v),\n",
        "        'quotes_emoji2vec': quotes_2v,\n",
        "        'avg_similarity_emojinal': np.mean(dists_ej),\n",
        "        'quotes_emojinal': quotes_ej\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EbAqqkHIlbz",
        "outputId": "30867360-9724-453b-abac-46999057042a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:31<00:00,  3.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Detailed Results ===\")\n",
        "for idx, res in enumerate(results[:10], 1):\n",
        "    print(f\"\\nQuery {idx}: {res['text']}\")\n",
        "    print(f\"[Emoji2Vec] Avg Cosine Similarity: {res['avg_similarity_emoji2vec']:.4f}\")\n",
        "    for i, q in enumerate(res['quotes_emoji2vec']):\n",
        "        print(f\"  {i+1}. {q}\")\n",
        "    print(f\"[Emojinal ] Avg Cosine Similarity: {res['avg_similarity_emojinal']:.4f}\")\n",
        "    for i, q in enumerate(res['quotes_emojinal']):\n",
        "        print(f\"  {i+1}. {q}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-4CKH4gXZDI",
        "outputId": "f04d1219-5340-473c-b568-93c7218a89d1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Detailed Results ===\n",
            "\n",
            "Query 1: @z388z @IFLTV Yh I had to search my house and take a picture of it ðŸ¤¡\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.1118\n",
            "  1. I can't not put humor in a book.\n",
            "  2. In an unforgiving world, chaos rules.\n",
            "  3. I wasn't going to have fun doing a teen movie again.\n",
            "  4. No one will laugh at how great things are for somebody.\n",
            "  5. I do not have a sense of humor of any recognizable sort.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.1217\n",
            "  1. I can't not put humor in a book.\n",
            "  2. It's hard to force creativity and humor.\n",
            "  3. No great thing is created suddenly.\n",
            "  4. No one will laugh at how great things are for somebody.\n",
            "  5. I wasn't going to have fun doing a teen movie again.\n",
            "\n",
            "Query 2: just booked hotel for 5sos in prague even when I'm not sure if i get my ticket \n",
            "SO hoping for good luck while ticketing ðŸ¤žðŸ¤žðŸ¤ž\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.0573\n",
            "  1. Even in the midst of the storm the sun is still shining.\n",
            "  2. Leadership is a skill learned through many venues.\n",
            "  3. The more you love, the more you awaken the divinity within.\n",
            "  4. True wisdom reveals itself by loving everyone.\n",
            "  5. Smile! Accept hatred with love.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.0488\n",
            "  1. Even in the midst of the storm the sun is still shining.\n",
            "  2. Self-esteem, self confidence\n",
            "  3. My mother used to say  \"He who angers you conquers you.\"\n",
            "  4. Win them with your infectious optimism and burning smile.\n",
            "  5. Everyday we can't connect but good things connected...\n",
            "\n",
            "Query 3: Stateside tips are live ðŸ‡ºðŸ‡¸\n",
            "\n",
            "âœ”ï¸ Tampa Bay Downs\n",
            "âœ”ï¸ Mahoning Valley\n",
            "âœ”ï¸ Philadelphia\n",
            "âœ”ï¸ Will Rogers Downs\n",
            "ðŸï¸ Turf Paradise\n",
            "\n",
            "ðŸ’° US Double pays 12/1!\n",
            "\n",
            "ðŸ“ PLUS 35/1 Notebook Double\n",
            "\n",
            "Get all my tips in America ðŸŒŽ\n",
            "https://t.co/tleEocHuYc\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.1068\n",
            "  1. Between optimism and pessimism, there is confidence in God.\n",
            "  2. A great many things keep happening, some good, some bad.\n",
            "  3. Gardening is an exercise in optimism.\n",
            "  4. Outwardly I was all confidence and openness\n",
            "  5. Well i am just going to try!\" she thought confidently!\n",
            "[Emojinal ] Avg Cosine Similarity: 0.1126\n",
            "  1. Between optimism and pessimism, there is confidence in God.\n",
            "  2. I do have faith in humanity but I donâ€™t have faith in humans.\n",
            "  3. Gardening is an exercise in optimism.\n",
            "  4. By faith you can work miracles and perform heroic deeds\n",
            "  5. My mind may be sober, but my confidence is high!\n",
            "\n",
            "Query 4: @Tofe906 20 ðŸ˜¨\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.1468\n",
            "  1. The basic success orientation is having an optimistic attitude.\n",
            "  2. Life ought to be live with great optimism.\n",
            "  3. Between optimism and pessimism, there is confidence in God.\n",
            "  4. Gardening is an exercise in optimism.\n",
            "  5. I'm pretty positive, optimistic, so I always expect the best.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.1424\n",
            "  1. The basic success orientation is having an optimistic attitude.\n",
            "  2. Life ought to be live with great optimism.\n",
            "  3. Gardening is an exercise in optimism.\n",
            "  4. Future:Life ahead is full of brighter days.\n",
            "  5. For success, optimism is more important than opportunity.\n",
            "\n",
            "Query 5: Order bento cake for ur twins yummy ðŸ˜‹ cakes https://t.co/HaqQgDO35Y\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.0934\n",
            "  1. Everything negative, useless, and redundant must go.\n",
            "  2. I donâ€™t like the situation but I must endure.\n",
            "  3. I am incapable of mediocrity.\n",
            "  4. Good enough never is\n",
            "  5. A victory which is not honourable is nothing but a defeat!\n",
            "[Emojinal ] Avg Cosine Similarity: 0.0966\n",
            "  1. ...we are overwhelmed with information and dying for wisdom.\n",
            "  2. Everything negative, useless, and redundant must go.\n",
            "  3. We are all potential of un-excited creativity\n",
            "  4. Everything is meaningless\n",
            "  5. Success is meaningless without fulfilment - both begin within\n",
            "\n",
            "Query 6: ðŸ˜‹ We're thrilled to report that our keto chocolate milk tea boba was a huge hit at the expo! \n",
            "\n",
            "ðŸ« Customers who sampled it raved about the delectable taste, with many likening it to the indulgent flavors of a fudgesicle. \n",
            "\n",
            "#boba #keto #ketoboba #bobatea #bobalife #bobamilktea https://t.co/H1Yq8q4HUF\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.0843\n",
            "  1. Being cool, having a 'cool' energy is just not attractive to me.\n",
            "  2. Wisdom is not wisdom when it is derived from books alone.\n",
            "  3. I do not admire greatness that has no substance.\n",
            "  4. There is no justice here\n",
            "  5. An undiscovered genius has no value in the marketplace\n",
            "[Emojinal ] Avg Cosine Similarity: 0.0875\n",
            "  1. Law applied to its extreme is the greatest injustice\n",
            "  2. Worship without honor is meaningless\n",
            "  3. I do not admire greatness that has no substance.\n",
            "  4. An undiscovered genius has no value in the marketplace\n",
            "  5. Being cool, having a 'cool' energy is just not attractive to me.\n",
            "\n",
            "Query 7: why leaving comments on my post with hashtags. y'all look like bots with copy paste.ðŸ˜‚\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.1491\n",
            "  1. Gardening is an exercise in optimism.\n",
            "  2. Push the needle into some middle range of guarded optimism.\n",
            "  3. Wherever you go, be the thunder of joy and the wind of optimism.\n",
            "  4. The basic success orientation is having an optimistic attitude.\n",
            "  5. The easy path will seldom lead where you need to go.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.1538\n",
            "  1. Gardening is an exercise in optimism.\n",
            "  2. Push the needle into some middle range of guarded optimism.\n",
            "  3. Wherever you go, be the thunder of joy and the wind of optimism.\n",
            "  4. The basic success orientation is having an optimistic attitude.\n",
            "  5. The easy path will seldom lead where you need to go.\n",
            "\n",
            "Query 8: @C0nstant_g God has his favoritesðŸ‘... Xd\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.1164\n",
            "  1. Worthless as wither'd weeds.\n",
            "  2. He who stops being better stops being good.\n",
            "  3. The man of science is a poor philosopher.\n",
            "  4. Without courage, wisdom bears no fruit.\n",
            "  5. To be wealthy and honored in an unjust society is a disgrace.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.1232\n",
            "  1. Without courage, wisdom bears no fruit.\n",
            "  2. Worthless as wither'd weeds.\n",
            "  3. It is ill to offer God one duty stained with the blood of another.\n",
            "  4. Youth is a failing only tooeasily outgrown.\n",
            "  5. Passion without knowledge and the desire to learn is foolishness\n",
            "\n",
            "Query 9: @backport893 Thank's ðŸ‘â¤ï¸\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.0869\n",
            "  1. Ã”, Sunlight! The most precious gold to be found on Earth.\n",
            "  2. The innocent curiosity of children gives them astounding courage.\n",
            "  3. Simple ideas INVOLVE individuals\n",
            "  4. Listen to me, youâ€™ve got this. Seriously, youâ€™ve got this.\n",
            "  5. Inspiration is everywhere.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.0892\n",
            "  1. Originality of mind untainted,And enthusiasm for life, unlimited.\n",
            "  2. Be solution oriented. Infect everyone with enthusiasm.\n",
            "  3. Ã”, Sunlight! The most precious gold to be found on Earth.\n",
            "  4. There is nothing greater than enthusiasm.\n",
            "  5. Society drives people crazy with lust and calls it advertising.\n",
            "\n",
            "Query 10: #TrustArmy Upd: the validation of your report is in full swing ðŸ¦¾\n",
            "\n",
            "Here's what we've done to speed up the process:\n",
            "âœ”ï¸ Onboarded the first validators \n",
            "\n",
            "âœ”ï¸ Developed a new approach to the validation process to eliminate the bottleneck discovered in Beta\n",
            "[Emoji2Vec] Avg Cosine Similarity: 0.0863\n",
            "  1. I do have faith in humanity but I donâ€™t have faith in humans.\n",
            "  2. Disappointment is the nurse of wisdom.\n",
            "  3. Disappointment is the nurse of wisdom.\n",
            "  4. Disappointment is the nurse of wisdom.\n",
            "  5. Friends and fire didnâ€™t go well together.\n",
            "[Emojinal ] Avg Cosine Similarity: 0.0919\n",
            "  1. Disappointment is the nurse of wisdom.\n",
            "  2. Disappointment is the nurse of wisdom.\n",
            "  3. Disappointment is the nurse of wisdom.\n",
            "  4. I do have faith in humanity but I donâ€™t have faith in humans.\n",
            "  5. The anthology was not a best-seller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis\n",
        "In this iteration, text and emoji were embedded separately and then combined via linear layers (not trained).\n",
        "\n",
        "Key Findings:\n",
        "\n",
        "* Cosine similarities dropped significantly (often < 0.15), much lower than in Iterations 1 and 2.\n",
        "\n",
        "* Some emotional context (e.g., sarcasm or joy) was captured better than in the demojized version.\n",
        "\n",
        "* No consistent winner between Emoji2Vec and Emojinal.\n",
        "\n",
        "* Untrained projection layers likely hurt performance â€” fusion was shallow and unoptimized."
      ],
      "metadata": {
        "id": "_Z7lKd7P_x5R"
      }
    }
  ]
}
