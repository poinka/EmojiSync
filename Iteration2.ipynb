{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd Iteration\n",
        "For this time we will use emoji.demojize to create text description from emoji text. The main purpose of this part is to evaluate is it enough to use this simple tool to capture emotional richness of emoji text.\n",
        "\n",
        "Main steps:\n",
        "1. Choose emoji-rich dataset\n",
        "2. Convert emojis to text descriptions (instead of removing)\n",
        "3. Create embeddings from this enriched text  \n",
        "4. Search most relevant quotes using FAISS index  \n",
        "5. Compare quality of retrieved quotes to baseline"
      ],
      "metadata": {
        "id": "fRb2xdMCH6ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAIHtEtgoVNO",
        "outputId": "4af1e935-a063-44b6-acee-dcebc652464f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/590.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f70aGjOMHx1T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import faiss\n",
        "from tabulate import tabulate\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare csv files"
      ],
      "metadata": {
        "id": "tBeszcq63XHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dfs = []\n",
        "for file_name in os.listdir(\"archive\"):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(\"archive\", file_name)\n",
        "        try:\n",
        "            # Read CSV with appropriate encoding\n",
        "            df = pd.read_csv(file_path)\n",
        "            all_dfs.append(df)\n",
        "            print(f\"Loaded {file_name} with {len(df)} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "print(f\"Combined dataset size: {len(combined_df)} rows\")\n",
        "print()\n",
        "combined_df.head()\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRZqZLJ7qdbz",
        "outputId": "a546a62b-10a9-4794-e4e2-67b3a5b21569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rabbit.csv with 20000 rows\n",
            "Loaded thinking_face.csv with 20000 rows\n",
            "Loaded hot_face.csv with 20000 rows\n",
            "Loaded smiling_face_with_heart-eyes.csv with 20000 rows\n",
            "Loaded fire.csv with 20000 rows\n",
            "Loaded folded_hands.csv with 20000 rows\n",
            "Loaded fearful_face.csv with 20000 rows\n",
            "Loaded rolling_on_the_floor_laughing.csv with 20000 rows\n",
            "Loaded saluting_face.csv with 20000 rows\n",
            "Loaded white_heart.csv with 20000 rows\n",
            "Loaded grinning_face_with_sweat.csv with 20000 rows\n",
            "Loaded thumbs_up.csv with 20000 rows\n",
            "Loaded cooking.csv with 20000 rows\n",
            "Loaded face_with_steam_from_nose.csv with 20000 rows\n",
            "Loaded rabbit_face.csv with 20000 rows\n",
            "Loaded sparkles.csv with 20000 rows\n",
            "Loaded smiling_face_with_halo.csv with 20000 rows\n",
            "Loaded ghost.csv with 20000 rows\n",
            "Loaded hatching_chick.csv with 20000 rows\n",
            "Error loading backhand_index_pointing_right.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Error loading red_heart.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Loaded partying_face.csv with 20001 rows\n",
            "Loaded smiling_face_with_sunglasses.csv with 20000 rows\n",
            "Loaded egg.csv with 20001 rows\n",
            "Loaded middle_finger.csv with 20000 rows\n",
            "Loaded clown_face.csv with 20000 rows\n",
            "Loaded melting_face.csv with 20000 rows\n",
            "Loaded smiling_face.csv with 20000 rows\n",
            "Loaded pile_of_poo.csv with 20000 rows\n",
            "Loaded sun.csv with 20000 rows\n",
            "Loaded check_mark_button.csv with 20000 rows\n",
            "Loaded smiling_face_with_hearts.csv with 20000 rows\n",
            "Loaded eyes.csv with 20000 rows\n",
            "Loaded face_with_tears_of_joy.csv with 20000 rows\n",
            "Loaded enraged_face.csv with 20000 rows\n",
            "Loaded loudly_crying_face.csv with 20000 rows\n",
            "Loaded face_holding_back_tears.csv with 20000 rows\n",
            "Loaded party_popper.csv with 20000 rows\n",
            "Loaded face_savoring_food.csv with 20000 rows\n",
            "Loaded skull.csv with 20000 rows\n",
            "Loaded smiling_face_with_tear.csv with 20000 rows\n",
            "Loaded winking_face.csv with 20000 rows\n",
            "Loaded check_mark.csv with 20000 rows\n",
            "Combined dataset size: 820002 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load dataset and filter rows with emojis"
      ],
      "metadata": {
        "id": "FK65VKfH3kjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_dataset.csv')\n",
        "df['has_emoji'] = df['Text'].apply(lambda x: bool(emoji.emoji_count(str(x))))\n",
        "emoji_rich_df = df[df['has_emoji']].copy()\n",
        "\n",
        "# Select 100 random records\n",
        "df_subset = emoji_rich_df.sample(n=100, random_state=100).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "D8y9AT89o4PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create embeddings of the text with emoji description"
      ],
      "metadata": {
        "id": "oKPpbCH74euR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Emoji to Text Description\n",
        "def demojize_text(text):\n",
        "    text = emoji.demojize(str(text), language='en')  # 😢 → \":crying_face:\"\n",
        "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
        "    return text"
      ],
      "metadata": {
        "id": "7ld8f-d_onuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "def get_embedding(text, tokenizer, model, device='cpu'):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states[-1]\n",
        "    cls_embedding = hidden_states[:, 0, :].squeeze().cpu().numpy()\n",
        "    return cls_embedding.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "GI_WQd7OoQsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Faiss index"
      ],
      "metadata": {
        "id": "O0zB2Kg-5Aiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_df = pd.read_csv('selected_quotes_embeddings.csv')\n",
        "\n",
        "def parse_embedding(emb):\n",
        "    try:\n",
        "        if isinstance(emb, str):\n",
        "            return np.array(eval(emb), dtype=np.float32)\n",
        "        return np.array(emb, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "index = faiss.IndexFlatIP(768)\n",
        "\n",
        "for i in range(0, len(quotes_df), 10000):\n",
        "    chunk = quotes_df.iloc[i:i + 10000]\n",
        "    chunk_embeddings = [parse_embedding(emb) for emb in chunk['embeddings']]\n",
        "    chunk_embeddings = [emb for emb in chunk_embeddings if emb is not None and emb.shape == (768,)]\n",
        "    if chunk_embeddings:\n",
        "        chunk_array = np.vstack(chunk_embeddings)\n",
        "        faiss.normalize_L2(chunk_array)\n",
        "        index.add(chunk_array)\n",
        "    print(f\"Processed chunk {i // 10000 + 1}/{len(quotes_df) // 10000 + 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFizTvxMqEOO",
        "outputId": "d609bbe4-16ab-4ec9-f229-20c453192fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1/11\n",
            "Processed chunk 2/11\n",
            "Processed chunk 3/11\n",
            "Processed chunk 4/11\n",
            "Processed chunk 5/11\n",
            "Processed chunk 6/11\n",
            "Processed chunk 7/11\n",
            "Processed chunk 8/11\n",
            "Processed chunk 9/11\n",
            "Processed chunk 10/11\n",
            "Processed chunk 11/11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_quotes(query_embedding, k=5):\n",
        "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
        "    distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "    return distances[0], indices[0]\n"
      ],
      "metadata": {
        "id": "HemKEHhuqGy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation"
      ],
      "metadata": {
        "id": "kYQ6wo7d5NRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for idx, row in df_subset.iterrows():\n",
        "    input_text = row['Text']\n",
        "\n",
        "    # Emojis turn into text\n",
        "    demojized_text = demojize_text(input_text)\n",
        "\n",
        "    demojized_embedding = get_embedding(demojized_text, tokenizer, model)\n",
        "    demojized_distances, demojized_indices = search_similar_quotes(demojized_embedding)\n",
        "    demojized_avg_similarity = np.mean(demojized_distances)\n",
        "    demojized_quotes = [quotes_df.iloc[i]['quote'] for i in demojized_indices]\n",
        "\n",
        "    results.append({\n",
        "        'text': input_text,\n",
        "        'demojized_text': demojized_text,\n",
        "        'demojized_avg_similarity': demojized_avg_similarity,\n",
        "        'demojized_quotes': demojized_quotes\n",
        "    })\n"
      ],
      "metadata": {
        "id": "9YRLAD86qHbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Detailed Results ===\")\n",
        "for idx, res in enumerate(results[:10], 1):\n",
        "    print(f\"\\nQuery {idx}: {res['text']}\")\n",
        "    print(f\"Demojized: {res['demojized_text']}\")\n",
        "    print(f\"Average Cosine Similarity: {res['demojized_avg_similarity']:.4f}\")\n",
        "    print(\"Top-K Quotes:\")\n",
        "    for i, quote in enumerate(res['demojized_quotes']):\n",
        "        print(f\"{i+1}. {quote}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4LDh-26qK51",
        "outputId": "4e64bacf-a144-4e5f-fbb2-aea03af48446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Detailed Results ===\n",
            "\n",
            "Query 1: @z388z @IFLTV Yh I had to search my house and take a picture of it 🤡\n",
            "Demojized: @z388z @IFLTV Yh I had to search my house and take a picture of it :clown_face:\n",
            "Average Cosine Similarity: 0.8694\n",
            "Top-K Quotes:\n",
            "1. Find me, my thief.\n",
            "2. Taking a dump...blackout\n",
            "3. They X-rayed my head and found nothing.\n",
            "4. Literature is a microscope\n",
            "5. ...a murder of crows gormandized until they were satiated.\n",
            "\n",
            "Query 2: just booked hotel for 5sos in prague even when I'm not sure if i get my ticket \n",
            "SO hoping for good luck while ticketing 🤞🤞🤞\n",
            "Demojized: just booked hotel for 5sos in prague even when I'm not sure if i get my ticket \n",
            "SO hoping for good luck while ticketing :crossed_fingers::crossed_fingers::crossed_fingers:\n",
            "Average Cosine Similarity: 0.8909\n",
            "Top-K Quotes:\n",
            "1. With hope, we can endure any hardship.\n",
            "2. \\with hope, you can survive any shock.\n",
            "3. To travel hopefully is better than to have arrived.\n",
            "4. With high hope and optimism, start swimming with time.\n",
            "5. There is hope in the challenge.\n",
            "\n",
            "Query 3: Stateside tips are live 🇺🇸\n",
            "\n",
            "✔️ Tampa Bay Downs\n",
            "✔️ Mahoning Valley\n",
            "✔️ Philadelphia\n",
            "✔️ Will Rogers Downs\n",
            "🏝️ Turf Paradise\n",
            "\n",
            "💰 US Double pays 12/1!\n",
            "\n",
            "📝 PLUS 35/1 Notebook Double\n",
            "\n",
            "Get all my tips in America 🌎\n",
            "https://t.co/tleEocHuYc\n",
            "Demojized: Stateside tips are live :United_States:\n",
            "\n",
            ":check_mark: Tampa Bay Downs\n",
            ":check_mark: Mahoning Valley\n",
            ":check_mark: Philadelphia\n",
            ":check_mark: Will Rogers Downs\n",
            ":desert_island: Turf Paradise\n",
            "\n",
            ":money_bag: US Double pays 12/1!\n",
            "\n",
            ":memo: PLUS 35/1 Notebook Double\n",
            "\n",
            "Get all my tips in America :globe_showing_Americas:\n",
            "https://t.co/tleEocHuYc\n",
            "Average Cosine Similarity: 0.9646\n",
            "Top-K Quotes:\n",
            "1. Eye Amost Evr Spel Ah Werd Wong Annymoe, sinc eye goat alto pel\n",
            "2. rip the prisonsopenput theconvictsontelevision\n",
            "3. Pietrisycamollaviadelrechiotemexity.\n",
            "4. Ahesta boro, Mah-e-man, ahesta boro.\n",
            "5. For Calo, Galdo, and Bug\n",
            "\n",
            "Query 4: @Tofe906 20 😨\n",
            "Demojized: @Tofe906 20 :fearful_face:\n",
            "Average Cosine Similarity: 0.9180\n",
            "Top-K Quotes:\n",
            "1. Fear clogs faith liberates. \n",
            "2. ...what the Man-Moth fears most he must do..\n",
            "3. Mind sees ghost when frightened and hopeless.\n",
            "4. To him who is in fear everything rustles.\n",
            "5. He discards a quilt for fear of bugs.\n",
            "\n",
            "Query 5: Order bento cake for ur twins yummy 😋 cakes https://t.co/HaqQgDO35Y\n",
            "Demojized: Order bento cake for ur twins yummy :face_savoring_food: cakes https://t.co/HaqQgDO35Y\n",
            "Average Cosine Similarity: 0.7656\n",
            "Top-K Quotes:\n",
            "1. If you like tiger cubs,you got to get into the den!\n",
            "2. I wait on my fix:I am a poetry junkie.\n",
            "3. Rhys was hot enough to bake cookies on.\n",
            "4. Lay up for yourselves treasures in heaven.\n",
            "5. My passionate leisure pursuit are reading, wondering and writing.\n",
            "\n",
            "Query 6: 😋 We're thrilled to report that our keto chocolate milk tea boba was a huge hit at the expo! \n",
            "\n",
            "🍫 Customers who sampled it raved about the delectable taste, with many likening it to the indulgent flavors of a fudgesicle. \n",
            "\n",
            "#boba #keto #ketoboba #bobatea #bobalife #bobamilktea https://t.co/H1Yq8q4HUF\n",
            "Demojized: :face_savoring_food: We're thrilled to report that our keto chocolate milk tea boba was a huge hit at the expo! \n",
            "\n",
            ":chocolate_bar: Customers who sampled it raved about the delectable taste, with many likening it to the indulgent flavors of a fudgesicle. \n",
            "\n",
            "#boba #keto #ketoboba #bobatea #bobalife #bobamilktea https://t.co/H1Yq8q4HUF\n",
            "Average Cosine Similarity: 0.8183\n",
            "Top-K Quotes:\n",
            "1. S.M.I.L.E - A Sweet Mile in Life's Experience\n",
            "2. Gratitude goes beyond a delight for a gift\n",
            "3. I got this delicious bottle of perfume called Fabreze\n",
            "4. He was everyone and every living creature in one ecstatic motion.\n",
            "5. The power of thoughts is ecstasy\n",
            "\n",
            "Query 7: why leaving comments on my post with hashtags. y'all look like bots with copy paste.😂\n",
            "Demojized: why leaving comments on my post with hashtags. y'all look like bots with copy paste.:face_with_tears_of_joy:\n",
            "Average Cosine Similarity: 0.7536\n",
            "Top-K Quotes:\n",
            "1. How can a bird that is born for joySit in a cage and sing?\n",
            "2. If ignorance is bliss  why aren't there more happy teenagers?\n",
            "3. When it comes to happiness, why would anyone want to delay it?\n",
            "4. Happy people plan actions  they don't plan results.\n",
            "5. Happy people plan actions  they don't plan results.\n",
            "\n",
            "Query 8: @C0nstant_g God has his favorites👍... Xd\n",
            "Demojized: @C0nstant_g God has his favorites:thumbs_up:... Xd\n",
            "Average Cosine Similarity: 0.8535\n",
            "Top-K Quotes:\n",
            "1. .burn, everybody likes to watch the fire..\n",
            "2. If you like tiger cubs,you got to get into the den!\n",
            "3. Spread, Foxy. Show me that sweet spot.\n",
            "4. The Anointing attracts Antagonism\n",
            "5. Southern DADDY—Dandy At Doin’ Diapers Y’all!\n",
            "\n",
            "Query 9: @backport893 Thank's 👍❤️\n",
            "Demojized: @backport893 Thank's :thumbs_up::red_heart:\n",
            "Average Cosine Similarity: 0.9255\n",
            "Top-K Quotes:\n",
            "1. Response to clapping: \"Thank you for ovating.\"\n",
            "2. You + Mesaw this...AMERICAThank you for finding America with me\n",
            "3. Very few writers thank their mothers for keen editorial insight\n",
            "4. Thanks To Those People Who Give A Taste Of Right Ways.\n",
            "5. Lord I thank you for the timely provisions\n",
            "\n",
            "Query 10: #TrustArmy Upd: the validation of your report is in full swing 🦾\n",
            "\n",
            "Here's what we've done to speed up the process:\n",
            "✔️ Onboarded the first validators \n",
            "\n",
            "✔️ Developed a new approach to the validation process to eliminate the bottleneck discovered in Beta\n",
            "Demojized: #TrustArmy Upd: the validation of your report is in full swing :mechanical_arm:\n",
            "\n",
            "Here's what we've done to speed up the process:\n",
            ":check_mark: Onboarded the first validators \n",
            "\n",
            ":check_mark: Developed a new approach to the validation process to eliminate the bottleneck discovered in Beta\n",
            "Average Cosine Similarity: 0.9008\n",
            "Top-K Quotes:\n",
            "1. Constant prayer quickly straightens out our thoughts.\n",
            "2. Discipline creates ways for the lovers of it.\n",
            "3. Education gives light.\n",
            "4. A ministry gives us the opportunity to establish roots\n",
            "5. Rituals help us change modes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis\n",
        "\n",
        "This iteration explores whether converting emojis to descriptive text (using `emoji.demojize`) helps retain emotional context in vector embeddings.\n",
        "\n",
        "Demojized tokens often improved interpretability of the tweet and led to better emotional alignment with retrieved quotes.\n",
        "For example, 🤞🤞🤞 became :crossed_fingers: and matched hopeful quotes like\n",
        "“With hope, we can endure any hardship.” However, some matches were still semantically shallow. For instance, the 😋-tagged tweet describing delicious food led to generic quotes like “My passionate leisure pursuit...”, missing the sensory joy or humor implied by the emoji.\n",
        "\n",
        "Results show that demojizing:\n",
        "- Preserves some of the emotional nuance lost in baseline\n",
        "- Improves average similarity for certain emojis\n",
        "- Still fails to fully capture sarcasm or context-aware emotion\n",
        "\n",
        "**Conclusion**: Demojize is a lightweight but limited improvement. Might benefit from more expressive models (e.g. emoji2vec or emojional) in future iterations.\n"
      ],
      "metadata": {
        "id": "5843h2vOqOi_"
      }
    }
  ]
}